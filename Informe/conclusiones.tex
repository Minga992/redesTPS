\section{Conclusiones}

Este fenómeno de que la mayoría de símbolos provean una cantidad información muy por encima de la entropía y sólo unos pocos por debajo, se presenta en todas las capturas. Si la entropía es una medida que indica la cantidad media de información por símbolo, ¿cómo se explica que tantos símbolos se encuentren muy por sobre la media y tan sólo una minoría por debajo? Esto puede explicarse dando un vistazo a las probabilidades de cada símbolo. Se puede ver en los resultados de las capturas que los símbolos que proveen la menor cantidad de información son aquellos con las probabilidades más altas y esto se condice con la teoría de la información, es decir, los sucesos con las probabilidades de ocurrencia más altas son los que aportan menos información (y viceversa), y a su vez si un suceso tiene 100\% de probabilidad de ocurrencia entonces se dice que su cantidad de información es $cero$. La entropía está directamente relacionada con la capacidad de distinguir símbolos, es decir, cuando la entropía es máxima todos los símbolos proveen la misma cantidad de información y son equiprobables, no hay ninguno que se distinga, y en cambio cuando los símbolos no son equiprobables y la entropía es baja se pueden distinguir aquellos símbolos que provean la menor cantidad de información, o sea, los que tengan las probabilidades más altas.